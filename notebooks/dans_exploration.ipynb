{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0165801b-dc9e-4b26-9c82-7e86881a7482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.4\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import feature_column as fc\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# set TF error log verbosity\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1acd988-4840-4d47-beee-8e52e25d6469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_col_lists(train):\n",
    "    LABEL_COLUMN = ['total_consumption_T']#define labels\n",
    "    ITERATIVE_HOLIDAY_COLS = ['holiday_T'] #define string columns (user_id)\n",
    "    STRING_COLS = ['user_id']\n",
    "    ITERATIVE_NUMERIC_COLS = ['min_temp_T','max_temp_T','total_consumption_T_minus_','day_of_year_sin_T0','day_of_year_cos_T0'] #define number columns\n",
    "    EXCLUDE_COLS = ['time']\n",
    "\n",
    "    numeric_cols = []\n",
    "    string_cols = []\n",
    "    holiday_cols = []\n",
    "    label_cols = []\n",
    "    exclusion_cols = []\n",
    "    for c in train.columns:\n",
    "        if any(substring in c for substring in ITERATIVE_NUMERIC_COLS):\n",
    "            numeric_cols.append(c)\n",
    "        if any(substring in c for substring in STRING_COLS):\n",
    "            string_cols.append(c)\n",
    "        if any(substring in c for substring in ITERATIVE_HOLIDAY_COLS):\n",
    "            holiday_cols.append(c)\n",
    "        if any((substring in c and 'minus' not in c) for substring in LABEL_COLUMN):\n",
    "            label_cols.append(c)\n",
    "        if any(substring in c  for substring in EXCLUDE_COLS):\n",
    "            exclusion_cols.append(c)\n",
    "    return numeric_cols, string_cols, holiday_cols, exclusion_cols, label_cols\n",
    "\n",
    "def features_and_labels(row_data):\n",
    "    \"\"\"Splits features and labels from feature dictionary.\n",
    "\n",
    "    Args:\n",
    "        row_data: Dictionary of CSV column names and tensor values.\n",
    "    Returns:\n",
    "        Dictionary of feature tensors and label tensor.\n",
    "    \"\"\"\n",
    "    for c in exclusion_cols:\n",
    "        row_data.pop(c)\n",
    "        \n",
    "    labels= []\n",
    "    for c in label_cols:\n",
    "        label_val = row_data.pop(c)\n",
    "        labels.append(label_val)\n",
    "    label = tf.stack(labels, axis=1)\n",
    "    return row_data, label  # features, label\n",
    "\n",
    "\n",
    "def load_dataset(pattern, columns, batch_size=1, mode=tf.estimator.ModeKeys.EVAL):\n",
    "    \"\"\"Loads dataset using the tf.data API from CSV files.\n",
    "\n",
    "    Args:\n",
    "        pattern: str, file pattern to glob into list of files.\n",
    "        batch_size: int, the number of examples per batch.\n",
    "        mode: tf.estimator.ModeKeys to determine if training or evaluating.\n",
    "    Returns:\n",
    "        `Dataset` object.\n",
    "    \"\"\"\n",
    "    # Make a CSV dataset\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern=pattern,\n",
    "        batch_size=batch_size,\n",
    "        column_names=columns)\n",
    "\n",
    "    # Map dataset to features and label\n",
    "    dataset = dataset.map(map_func=features_and_labels)  # features, label\n",
    "\n",
    "    # Shuffle and repeat for training\n",
    "    # if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    #     dataset = dataset.shuffle(buffer_size=1000).repeat()\n",
    "\n",
    "    # Take advantage of multi-threading; 1=AUTOTUNE\n",
    "    dataset = dataset.prefetch(buffer_size=1)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def create_input_layers():\n",
    "        #INPUT LAYER\n",
    "    inputs = {\n",
    "        colname: layers.Input(name=colname, shape=(), dtype='float32')\n",
    "        for colname in numeric_cols\n",
    "    }\n",
    "    inputs2 = {\n",
    "        colname: layers.Input(name=colname, shape=(), dtype='string')\n",
    "        for colname in holiday_cols\n",
    "    }\n",
    "    inputs3 = {\n",
    "        colname: layers.Input(name=colname, shape=(), dtype='string')\n",
    "        for colname in string_cols\n",
    "    }\n",
    "\n",
    "    inputs.update(inputs2)\n",
    "    inputs.update(inputs3)\n",
    "    return inputs\n",
    "\n",
    "def create_feature_columns(user_ids):\n",
    "    #FEATURE COLUMNS\n",
    "    feature_columns = []\n",
    "    # numeric cols\n",
    "    feature_columns = {\n",
    "        colname : fc.numeric_column(key=colname)\n",
    "               for colname in numeric_cols\n",
    "    }\n",
    "    feature_columns2 = {\n",
    "        colname : fc.indicator_column(fc.categorical_column_with_vocabulary_list(key=colname, vocabulary_list=['no holiday', 'minor', 'major']))\n",
    "               for colname in holiday_cols\n",
    "    }\n",
    "    feature_columns3 = {\n",
    "        colname : fc.indicator_column(fc.categorical_column_with_vocabulary_list(key=colname, vocabulary_list=user_ids))\n",
    "               for colname in string_cols\n",
    "    }\n",
    "\n",
    "    feature_columns.update(feature_columns2)\n",
    "    feature_columns.update(feature_columns3)\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92076545-c935-4a2f-848a-a93f97315278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_outputs(inputs):\n",
    "    \"\"\"Creates model architecture and returns outputs.\n",
    "\n",
    "    Args:\n",
    "        inputs: Dense tensor used as inputs to model.\n",
    "    Returns:\n",
    "        Dense tensor output from the model.\n",
    "    \"\"\"\n",
    "    # Create two hidden layers of [64, 32] just in like the BQML DNN\n",
    "    h1 = tf.keras.layers.Dense(300, activation=\"relu\", name=\"h1\")(inputs)\n",
    "    h2 = tf.keras.layers.Dense(150, activation=\"relu\", name=\"h2\")(h1)\n",
    "    h3 = tf.keras.layers.Dense(84, activation=\"relu\", name=\"h3\")(h2)\n",
    "    h4 = tf.keras.layers.Dense(42, activation=\"relu\", name=\"4h\")(h3)\n",
    "\n",
    "    # Final output is a linear activation because this is regression\n",
    "    output = tf.keras.layers.Dense(\n",
    "        units=28, activation=\"linear\", name=\"weight\")(h4)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d890be-db9b-44e7-939f-8274a833d6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"Calculates RMSE evaluation metric.\n",
    "\n",
    "    Args:\n",
    "        y_true: tensor, true labels.\n",
    "        y_pred: tensor, predicted labels.\n",
    "    Returns:\n",
    "        Tensor with value of RMSE between true and predicted labels.\n",
    "    \"\"\"\n",
    "    return tf.sqrt(tf.reduce_mean((y_pred - y_true) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2854f50e-e2a6-47fb-bc22-3d16f3b62a29",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dnn_model(user_ids):\n",
    "    \"\"\"Builds simple DNN using Keras Functional API.\n",
    "\n",
    "    Returns:\n",
    "        `tf.keras.models.Model` object.\n",
    "    \"\"\"\n",
    "    # Create input layer\n",
    "    inputs = create_input_layers()\n",
    "\n",
    "    # Create feature columns\n",
    "    feature_columns = create_feature_columns(user_ids)\n",
    "\n",
    "    # The constructor for DenseFeatures takes a list of numeric columns\n",
    "    # The Functional API in Keras requires: LayerConstructor()(inputs)\n",
    "    dnn_inputs = tf.keras.layers.DenseFeatures(\n",
    "        feature_columns=feature_columns.values())(inputs)\n",
    "\n",
    "    # Get output of model given inputs\n",
    "    output = get_model_outputs(dnn_inputs)\n",
    "\n",
    "    # Build model and compile it all together\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[rmse, \"mse\"])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf1e8681-0115-4e70-8c30-f821ab028213",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train_energy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da31581-d276-43b2-b7bc-0c0b390a9f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 17:07:17.174363: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2299995000 Hz\n",
      "2021-10-29 17:07:17.174739: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557203e8ab60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-10-29 17:07:17.174763: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-10-29 17:07:17.176946: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2021-10-29 17:07:19.681966: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:543: UserWarning: Input dict contained keys ['day_pod', 'prediction_window_T0', 'day_of_prediction'] which did not match any model input. They will be ignored by the model.\n",
      "  [n for n in tensors.keys() if n not in ref_input_names])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/312 [..............................] - ETA: 13s - loss: 1773114112.0000 - rmse: 30854.0137 - mse: 1773114112.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 17:07:32.226533: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\n",
      "2021-10-29 17:07:32.283303: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/20211029-170719/train/plugins/profile/2021_10_29_17_07_32\n",
      "2021-10-29 17:07:32.287189: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/20211029-170719/train/plugins/profile/2021_10_29_17_07_32/daniel-dlvm.trace.json.gz\n",
      "2021-10-29 17:07:32.305653: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/20211029-170719/train/plugins/profile/2021_10_29_17_07_32\n",
      "2021-10-29 17:07:32.306161: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/20211029-170719/train/plugins/profile/2021_10_29_17_07_32/daniel-dlvm.memory_profile.json.gz\n",
      "2021-10-29 17:07:32.306848: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/20211029-170719/train/plugins/profile/2021_10_29_17_07_32Dumped tool data for xplane.pb to logs/20211029-170719/train/plugins/profile/2021_10_29_17_07_32/daniel-dlvm.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/20211029-170719/train/plugins/profile/2021_10_29_17_07_32/daniel-dlvm.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/20211029-170719/train/plugins/profile/2021_10_29_17_07_32/daniel-dlvm.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/20211029-170719/train/plugins/profile/2021_10_29_17_07_32/daniel-dlvm.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/20211029-170719/train/plugins/profile/2021_10_29_17_07_32/daniel-dlvm.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 29s 92ms/step - loss: 3852111104.0000 - rmse: 34774.0742 - mse: 3852111104.0000 - val_loss: 2834700288.0000 - val_rmse: 48537.2617 - val_mse: 2834700288.0000\n",
      "Epoch 2/5\n",
      "312/312 [==============================] - 23s 73ms/step - loss: 1283068416.0000 - rmse: 20325.8691 - mse: 1283068416.0000 - val_loss: 1460335872.0000 - val_rmse: 33031.5195 - val_mse: 1460335872.0000\n",
      "Epoch 3/5\n",
      "312/312 [==============================] - 24s 76ms/step - loss: 1357129728.0000 - rmse: 19413.2227 - mse: 1357129728.0000 - val_loss: 965727808.0000 - val_rmse: 29127.9434 - val_mse: 965727808.0000\n",
      "Epoch 4/5\n",
      "312/312 [==============================] - 23s 74ms/step - loss: 1251340544.0000 - rmse: 19702.5352 - mse: 1251340544.0000 - val_loss: 4274119168.0000 - val_rmse: 62234.1641 - val_mse: 4274119168.0000\n",
      "Epoch 5/5\n",
      "311/312 [============================>.] - ETA: 0s - loss: 2562459904.0000 - rmse: 22861.1504 - mse: 2562459904.0000"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE = 32\n",
    "TEST_BATCH_SIZE = 1000\n",
    "NUM_TRAIN_EXAMPLES = 10000 * 5  # training dataset repeats, it'll wrap around\n",
    "NUM_EVALS = 5  # how many times to evaluate\n",
    "# Enough to get a reasonable sample, but not so much that it slows down\n",
    "NUM_EVAL_EXAMPLES = 10000\n",
    "\n",
    "\n",
    "\n",
    "numeric_cols, string_cols, holiday_cols, exclusion_cols, label_cols = generate_col_lists(train)\n",
    "\n",
    "trainds = load_dataset(\n",
    "    pattern=\"./data/train*\",\n",
    "    columns=train.columns,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    mode=tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "evalds = load_dataset(\n",
    "    pattern=\"./data/test*\",\n",
    "    columns=train.columns,\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    mode=tf.estimator.ModeKeys.EVAL).take(count=NUM_EVAL_EXAMPLES // 1000)\n",
    "\n",
    "steps_per_epoch = NUM_TRAIN_EXAMPLES // (TRAIN_BATCH_SIZE * NUM_EVALS)\n",
    "\n",
    "logdir = os.path.join(\n",
    "    \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=logdir, histogram_freq=1)\n",
    "\n",
    "\n",
    "model = build_dnn_model(train.user_id.unique())\n",
    "\n",
    "history = model.fit(\n",
    "    trainds,\n",
    "    validation_data=evalds,\n",
    "    epochs=NUM_EVALS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b9762-ca79-4e7c-8990-d568d0aaa843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m81"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
